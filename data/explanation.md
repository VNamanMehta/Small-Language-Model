# Data Directory Explanation

## Overview
The `data` directory handles all data preparation steps: downloading raw data, tokenization, and creating processed datasets ready for model training.

## Files and Their Purpose

### 1. **ingestion.py**
**Purpose**: Downloads and collects raw text data from various sources.

**Why It's Required**:
- Centralizes data collection in one place
- Handles different data formats and sources
- Validates data integrity before processing

**Key Responsibilities**:
- **Source Handling**: Can fetch from:
  - Local files
  - URLs/HTTP endpoints
  - APIs
  - Publicly available datasets
- **Format Conversion**: Handles txt, csv, json, parquet formats
- **Validation**: Checks data quality, handles duplicates
- **Output**: Saves raw text to `raw/` directory

**Why Separate Ingestion?**
- Keeps data collection logic isolated
- Easy to add new data sources without affecting other components
- Reproducible data collection process

---

### 2. **train_tokenizer.py**
**Purpose**: Builds a vocabulary and trains the tokenizer on raw data.

**Why It's Required**:
- Converts text into numerical tokens the model can process
- Creates vocabulary of unique tokens
- Essential preprocessing step before training

**Theory - What is Tokenization?**

**Character-Level Tokenization**: Each character is a token
- Pros: Small vocabulary (127 ASCII chars), can handle any text
- Cons: Long sequences, hard to learn word meanings

**Word-Level Tokenization**: Each word is a token
- Pros: Semantic units, shorter sequences
- Cons: Huge vocabulary (100K+ words), cannot handle misspellings

**Subword Tokenization (BPE - Byte Pair Encoding)**: 
- **How It Works**:
  1. Start with character-level tokens
  2. Find most frequent character pair, merge into one token
  3. Repeat until vocabulary reaches target size
  4. Example: `hello` → `h`, `e`, `ll`, `o` → vocabulary grows dynamically
- **Why BPE?**
  - Balances vocabulary size and sequence length
  - Handles unknown words by breaking them into subwords
  - Used in GPT, BERT, modern language models

**Key Tokenizer Concepts**:
- **Vocabulary**: Set of all unique tokens (size ~10K-50K typically)
- **Token ID**: Numerical representation of each token
- **Special Tokens**:
  - `<PAD>`: Padding (for fixed-length sequences)
  - `<UNK>`: Unknown token (for rare/unseen words)
  - `<BOS>`: Beginning of sequence
  - `<EOS>`: End of sequence
  - `<CLS>`: Classification token

**Tokenizer Output**:
- Vocabulary file: Maps token ID ↔ token string
- Token frequencies: Used for analysis
- Configuration: Stores tokenizer settings for reproducibility

---

### 3. **prepare_data.py**
**Purpose**: Processes raw text data into training-ready format.

**Why It's Required**:
- Cleans and standardizes data
- Applies tokenization
- Creates data splits (train/validation/test)
- Saves in efficient format for fast loading

**Processing Pipeline**:

```
Raw Text Files
    ↓
[Clean Text]
├─ Remove special characters (if needed)
├─ Handle encoding issues
├─ Normalize whitespace
└─ Case conversion (if needed)
    ↓
[Tokenize]
├─ Apply trained tokenizer
├─ Convert text → token IDs
└─ Handle out-of-vocabulary words
    ↓
[Create Splits]
├─ 80% Training data
├─ 10% Validation data
└─ 10% Test data
    ↓
[Save Processed Data]
├─ Format: .npy, .pt, or .bin (binary for efficiency)
├─ Metadata: lengths, split info
└─ Ready for DataLoader
```

**Why These Steps?**

**Cleaning**:
- Removes noise that confuses the model
- Handles encoding issues (UTF-8 errors)
- Ensures consistency across datasets

**Tokenization**:
- Converts text → numbers (only form model understands)
- Reduces memory footprint (token ID < full character)

**Train/Val/Test Split**:
- **Training Set (80%)**: Used for weight updates
- **Validation Set (10%)**: Measures generalization during training; early stopping
- **Test Set (10%)**: Final evaluation on completely unseen data

**Why This Split?**
- Prevents overfitting (model memorizing training data)
- Validates generalization
- Provides honest final performance estimate

**Efficient Formats**:
- **NumPy (.npy)**: Fast loading for large arrays
- **PyTorch (.pt)**: Direct compatibility with training code
- **Binary formats**: Compact storage, minimal memory overhead

---

## Directory Structure

### `raw/`
**Purpose**: Stores original unprocessed data.
- Downloaded from sources via `ingestion.py`
- Never modified (immutable source of truth)
- Can be recreated from ingestion process

### `processed/`
**Purpose**: Stores cleaned, tokenized, train/val/test split data.
- Generated by `prepare_data.py`
- Used by DataLoader during training
- Can be deleted and regenerated if needed

### `tokenizer/`
**Purpose**: Stores trained tokenizer artifacts.
- **vocab.json**: Token ID ↔ token string mapping
- **merges.txt**: BPE merge operations (for reproduction)
- **config.json**: Tokenizer settings
- **Special tokens**: Definitions of `<PAD>`, `<UNK>`, etc.

**Why Save Tokenizer?**
- Ensures consistent tokenization across train/inference
- Can load same tokenizer for text generation
- Cannot recreate identical tokenizer from raw data alone (statistical process)

---

## Data Preparation Workflow

```
Step 1: Ingestion (ingestion.py)
├─ Download/collect text data
├─ Validate format
└─ → raw/ directory

Step 2: Train Tokenizer (train_tokenizer.py)
├─ Read raw text
├─ Build vocabulary (BPE algorithm)
├─ Learn merge operations
└─ → tokenizer/ directory

Step 3: Prepare Data (prepare_data.py)
├─ Read raw text + tokenizer
├─ Clean and tokenize
├─ Split into train/val/test
├─ Convert to efficient format
└─ → processed/ directory

Output Ready for Training!
```

---

## Why This Data Architecture?

1. **Separation of Concerns**: Each file handles one task
2. **Reproducibility**: Can regenerate any step from raw data
3. **Efficiency**: Processed data loads quickly during training
4. **Scalability**: Easy to add new data sources in ingestion
5. **Tokenization Consistency**: Saved tokenizer ensures identical encoding in production

---

## Key Data Concepts

### Vocabulary Size Trade-off
- **Larger vocab**: Fewer subword tokens, but larger embeddings and output layer
- **Smaller vocab**: More subword tokens in sequences, smaller model size

### Token Frequency Distribution
- Most common tokens (the, a, is) appear millions of times
- Many rare tokens appear only once
- Creates highly imbalanced distribution - important for model to learn

### Sequence Length Considerations
- **Fixed Length**: Pad or truncate to same length; simpler but wastes computation
- **Variable Length**: Use attention masks; more efficient but complex

### Validation Split Importance
- If model trains and validates on same distribution but tests on different data
- Model won't generalize (high train loss, high test loss)
- Validation catches this early during training

The data directory ensures high-quality, properly formatted data flows into the training pipeline!