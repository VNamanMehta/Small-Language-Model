# TinyStories SLM : A Minimalist Language Model

This project is a from-scratch implementation of a GPT-style small language model trained on the "TinyStories" dataset.

## Features

*   GPT-style transformer model.
*   Custom BPE tokenizer trained on the TinyStories dataset.
*   Parallelized data preparation pipeline using `multiprocessing` and `numpy.memmap`.
*   Training script with checkpointing and auto-resuming capabilities.
*   Generation script to sample stories from a trained model.
*   Evaluation script to measure validation loss and perplexity.

## Installation

1.  **Clone the repository:**
    ```bash
    git clone <repository_url>
    cd <repository_directory>
    ```

2.  **Create and activate a virtual environment:**
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows, use `venv\Scripts\activate`
    ```

3.  **Install the dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

## Usage: Step-by-Step Workflow

To train and use the model, follow these steps in order:

**1. Download the Data**

This script downloads the "TinyStories" dataset from the Hugging Face Hub and saves it to `data/raw`.

```bash
python data/ingestion.py
```

**2. Train the Tokenizer**

This script trains a BPE tokenizer on a subset of the downloaded data and saves it to `data/tokenizer/tokenizer.json`.

```bash
python data/train_tokenizer.py
```

**3. Prepare the Data**

This script tokenizes the entire dataset using the trained tokenizer and packs it into efficient binary files (`train.bin` and `validation.bin`) in the `data/processed` directory. This step is parallelized to use all available CPU cores and can take a few minutes.

```bash
python data/prepare_data.py
```

**4. Train the Model**

This is the main training script. It will train the model and save checkpoints to the `checkpoints/` directory. If a checkpoint is found, it will automatically resume training from there.

```bash
python train.py
```

**5. Generate Stories**

Once you have a trained checkpoint, you can use this script to generate new stories.

```bash
python generate.py
```
<details>
<summary>Sample Output</summary>

```
‚è≥ Loading model from checkpoints/ckpt_36000.pt...
‚úÖ Model loaded successfully.

üìù Prompt: Once upon a time, there was a huge dragon
ü§ñ Generating...
--------------------------------------------------
Once upon a time, there was a huge dragon. The dragon was very friendly. He loved to play with the other animals in the forest. One day, the dragon was playing with a little bird. The bird was singing a song. The dragon liked the song and started to dance. The bird was very happy to see the dragon dance. They played together all day long.
--------------------------------------------------
```
</details>

**6. Evaluate the Model**

To evaluate a trained checkpoint on the validation set, run:

```bash
python evaluate.py
```

By default, it will use the latest checkpoint. To evaluate a specific one, use the `--checkpoint` flag:

```bash
python evaluate.py --checkpoint checkpoints/ckpt_10000.pt
```
<details>
<summary>Sample Output</summary>

```
üî• Evaluating on Device: CUDA
No checkpoint specified. Searching for the latest one...
‚è≥ Loading model from checkpoints\ckpt_36000.pt...
‚úÖ Model loaded successfully.
Evaluating for 100 iterations...
Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.95it/s]

==============================
‚ú® Evaluation Results ‚ú®
Checkpoint: checkpoints\ckpt_36000.pt
Average Validation Loss: 1.3461
Perplexity: 3.8425
==============================
```
</details>

## Model Architecture

The model is a standard GPT-style decoder-only transformer with the following default parameters:

*   **Embedding Dimension:** 256
*   **Number of Layers:** 8
*   **Number of Heads:** 8
*   **Vocabulary Size:** 10,000
*   **Max Sequence Length:** 256
*   **Total Parameters:** ~9 Million

## Dataset

This model is trained on the [TinyStories dataset](https://huggingface.co/datasets/roneneldan/TinyStories), a collection of short stories generated by GPT-3.5 and GPT-4 that are designed to be simple enough for small language models to learn from.
