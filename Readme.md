# TinyStories GPT: A Minimalist Language Model

This project is a from-scratch implementation of a GPT-style small language model trained on the "TinyStories" dataset.

## Features

*   GPT-style transformer model.
*   Custom BPE tokenizer trained on the TinyStories dataset.
*   Parallelized data preparation pipeline using `multiprocessing` and `numpy.memmap`.
*   Training script with checkpointing and auto-resuming capabilities.
*   Generation script to sample stories from a trained model.
*   Evaluation script to measure validation loss and perplexity.

## Installation

1.  **Clone the repository:**
    ```bash
    git clone <repository_url>
    cd <repository_directory>
    ```

2.  **Create and activate a virtual environment:**
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows, use `venv\Scripts\activate`
    ```

3.  **Install the dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

## Usage: Step-by-Step Workflow

To train and use the model, follow these steps in order:

**1. Download the Data**

This script downloads the "TinyStories" dataset from the Hugging Face Hub and saves it to `data/raw`.

```bash
python data/ingestion.py
```

**2. Train the Tokenizer**

This script trains a BPE tokenizer on a subset of the downloaded data and saves it to `data/tokenizer/tokenizer.json`.

```bash
python data/train_tokenizer.py
```

**3. Prepare the Data**

This script tokenizes the entire dataset using the trained tokenizer and packs it into efficient binary files (`train.bin` and `validation.bin`) in the `data/processed` directory. This step is parallelized to use all available CPU cores and can take a few minutes.

```bash
python data/prepare_data.py
```

**4. Train the Model**

This is the main training script. It will train the model and save checkpoints to the `checkpoints/` directory. If a checkpoint is found, it will automatically resume training from there.

```bash
python train.py
```

**5. Generate Stories**

Once you have a trained checkpoint, you can use this script to generate new stories.

```bash
python generate.py
```

You can customize the prompt and other generation parameters by editing the `generate.py` file.

**6. Evaluate the Model**

To evaluate a trained checkpoint on the validation set, run:

```bash
python evaluate.py
```

By default, it will use the latest checkpoint. To evaluate a specific one, use the `--checkpoint` flag:

```bash
python evaluate.py --checkpoint checkpoints/ckpt_10000.pt --prompt "Once upon a time"
```

## Model Architecture

The model is a standard GPT-style decoder-only transformer with the following default parameters:

*   **Embedding Dimension:** 256
*   **Number of Layers:** 8
*   **Number of Heads:** 8
*   **Vocabulary Size:** 10,000
*   **Max Sequence Length:** 256

## Dataset

This model is trained on the [TinyStories dataset](https://huggingface.co/datasets/roneneldan/TinyStories), a collection of short stories generated by GPT-3.5 and GPT-4 that are designed to be simple enough for small language models to learn from.